{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前言\n",
    "这个baseline是参考某大佬[github开源](https://github.com/biaobiao2/DC_phone/blob/master/code/baseline.py)    \n",
    "在开源基础上进行一些简单的修改，线上实际可达0.92+     \n",
    "具体.92的模型或参数较乱，可能线上达不到.92，各位大佬自行探索一下     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import stats\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "pd.set_option('display.width',1000)\n",
    "import os\n",
    "os.chdir(\"F:/MyCode/SiChuan/诈骗电话识别/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feats(df_train,df_test):\n",
    "    '''\n",
    "    @Param：\n",
    "        训练集和测试集的user表\n",
    "    @Function：\n",
    "        对地区特征的处理采用坏样本率替换，取消原baseline的编码替换\n",
    "    @Return：\n",
    "        返回地区处理后user表\n",
    "    '''\n",
    "    print('USER')\n",
    "    # 空值替换为未知地域\n",
    "    df_train[['city_name','county_name']] = df_train[['city_name','county_name']].fillna('未知')\n",
    "    df_test[['city_name','county_name']] = df_test[['city_name','county_name']].fillna('未知')\n",
    "    \n",
    "    # 计算地区的坏样本率：地区坏样本率为某地区的坏样本占该地区所有样本的比值\n",
    "    # 以及整体的坏样本率：整体坏样本率为该地区的坏样本占所有样本的比值\n",
    "    city_1 = df_train.groupby(['city_name','label'])['label'].count()[:,1] \\\n",
    "        / df_train.groupby('city_name')['label'].count()\n",
    "    city_2 = df_train.groupby(['city_name','label'])['label'].count()[:,1] \\\n",
    "        / df_train.groupby('label')['city_name'].count()[1]\n",
    "    \n",
    "    df_train[\"city_name_1\"] = df_train[\"city_name\"].replace(city_1.to_dict())\n",
    "    df_train[\"city_name_2\"] = df_train[\"city_name\"].replace(city_2.to_dict())\n",
    "    df_test[\"city_name_1\"] = df_test[\"city_name\"].replace(city_1.to_dict())\n",
    "    df_test[\"city_name_2\"] = df_test[\"city_name\"].replace(city_2.to_dict())\n",
    "    \n",
    "    # 计算分公司的坏样本率：同上\n",
    "    # 及整体的坏样本率：同上\n",
    "    county_1 = df_train.groupby(['county_name','label'])['label'].count()[:,1] \\\n",
    "        / df_train.groupby('county_name')['label'].count()\n",
    "    county_2 = df_train.groupby(['county_name','label'])['label'].count()[:,1] \\\n",
    "        / df_train.groupby('label')['county_name'].count()[1]\n",
    "    \n",
    "    # 对于county_2计算中无诈骗电话的分公司在county_2中未出现，用0进行填充\n",
    "    ind = [x for x in county_1.index if x not in county_2.index]\n",
    "    ind1 = [x for x in set(df_test.county_name) if x not in list(df_train.county_name)]\n",
    "    # 在测试集中有部分分公司未在训练集中出现，无法采集坏样本率同样用0填充\n",
    "    ind = ind + ind1\n",
    "    ser = pd.Series(0,index=ind)\n",
    "    county_2 = county_2.append(ser)\n",
    "    county_1 = county_1.append(pd.Series(0,index=ind1))\n",
    "    county_1 = county_1.fillna(0)\n",
    "    \n",
    "    df_train[\"county_name_1\"] = df_train[\"county_name\"].replace(county_1.to_dict())\n",
    "    df_train[\"county_name_2\"] = df_train[\"county_name\"].replace(county_2.to_dict())\n",
    "    df_test[\"county_name_1\"] = df_test[\"county_name\"].replace(county_1.to_dict())\n",
    "    df_test[\"county_name_2\"] = df_test[\"county_name\"].replace(county_2.to_dict())\n",
    "    \n",
    "    df_train.drop(['city_name','county_name'],axis=1,inplace=True)\n",
    "    df_test.drop(['city_name','county_name'],axis=1,inplace=True)\n",
    "    \n",
    "    return df_train,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流量统计\n",
    "def get_app_feats(df):\n",
    "    '''\n",
    "    @Function：\n",
    "        流量统计函数，该函数在原有baseline基础上,选用3月份数据\n",
    "    '''\n",
    "    print('APP')\n",
    "    df = df[df['month_id'] == '2020-03']\n",
    "    phones_app = df[[\"phone_no_m\"]].copy()\n",
    "    phones_app = phones_app.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"busi_name\"].agg(busi_count=\"nunique\")\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    '''\n",
    "    使用的流量统计\n",
    "    '''\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"flow\"].agg(flow_mean=\"mean\", \n",
    "                                               flow_median = \"median\", \n",
    "                                               flow_min  = \"min\", \n",
    "                                               flow_max = \"max\", \n",
    "                                               flow_var = \"var\",\n",
    "                                               flow_sum = \"sum\")\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"month_id\"].agg(month_ids =\"nunique\")\n",
    "    phones_app = phones_app.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    # 月流量使用统计\n",
    "    phones_app[\"flow_month\"] = phones_app[\"flow_sum\"] / phones_app[\"month_ids\"]\n",
    "\n",
    "    return phones_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通话记录统计\n",
    "def get_voc_feat(df):\n",
    "    '''\n",
    "    @Function：\n",
    "        在原有baseline基础上加入week统计指标，同时只选取3月份数据\n",
    "        参考2016诈骗电话活动规律与行为特征分析报告，指出诈骗电话周一到周五活跃度较高\n",
    "        \n",
    "        通话记录还有很多其他强特有待挖掘\n",
    "    '''\n",
    "    print('VOC')\n",
    "    df[\"start_datetime\"] = pd.to_datetime(df['start_datetime'])\n",
    "    df = df[df['start_datetime'] >= '2020-03-01 00:00:00']\n",
    "    df[\"hour\"] = df['start_datetime'].dt.hour\n",
    "    df[\"day\"] = df['start_datetime'].dt.day\n",
    "    df[\"week\"] = df['start_datetime'].dt.weekday\n",
    "    \n",
    "    # print(df.head())\n",
    "    phone_no_m = df[[\"phone_no_m\"]].copy()\n",
    "    phone_no_m = phone_no_m.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    #对话人数和对话次数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"opposite_no_m\"].agg(opposite_count=\"count\", opposite_unique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \"\"\"主叫通话\n",
    "    \"\"\"\n",
    "    df_call = df[df[\"calltype_id\"]==1].copy()\n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"imei_m\"].agg(voccalltype1=\"count\", imeis=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    phone_no_m[\"voc_calltype1\"] = phone_no_m[\"voccalltype1\"] / phone_no_m[\"opposite_count\"] \n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"city_name\"].agg(city_name_call=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    tmp = df_call.groupby(\"phone_no_m\")[\"county_name\"].agg(county_name_call=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    \"\"\"和固定通话者的对话统计\n",
    "    \"\"\"\n",
    "    tmp = df.groupby([\"phone_no_m\",\"opposite_no_m\"])[\"call_dur\"].agg(count=\"count\", sum=\"sum\")\n",
    "    phone2opposite = tmp.groupby(\"phone_no_m\")[\"count\"].agg(phone2opposite_mean=\"mean\", phone2opposite_median=\"median\", phone2opposite_max=\"max\")\n",
    "    phone_no_m = phone_no_m.merge(phone2opposite, on=\"phone_no_m\", how=\"left\")\n",
    "    phone2opposite = tmp.groupby(\"phone_no_m\")[\"sum\"].agg(phone2oppo_sum_mean=\"mean\", phone2oppo_sum_median=\"median\", phone2oppo_sum_max=\"max\")\n",
    "    phone_no_m = phone_no_m.merge(phone2opposite, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    \"\"\"通话时间长短统计\n",
    "    \"\"\"\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"call_dur\"].agg(call_dur_mean=\"mean\", call_dur_median=\"median\", call_dur_max=\"max\", call_dur_min=\"min\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby(\"phone_no_m\")[\"city_name\"].agg(city_name_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"county_name\"].agg(county_name_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"calltype_id\"].agg(calltype_id_unique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    \"\"\"通话时间点偏好\n",
    "    \"\"\"\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"hour\"].agg(voc_hour_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               voc_hour_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               voc_hour_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby(\"phone_no_m\")[\"day\"].agg(voc_day_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               voc_day_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               voc_day_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby(\"phone_no_m\")[\"week\"].agg(voc_week_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               voc_week_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               voc_week_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    return phone_no_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 短信记录统计\n",
    "def get_sms_feats(df):\n",
    "    '''\n",
    "    @Function：\n",
    "        在原有baseline基础上同样选取3月份数据，同样加入week特征\n",
    "    '''\n",
    "    print('SMS')\n",
    "    df['request_datetime'] = pd.to_datetime(df['request_datetime'] )\n",
    "    df = df[df['request_datetime'] >= '2020-03-01 00:00:00']\n",
    "\n",
    "    \n",
    "    df[\"hour\"] = df['request_datetime'].dt.hour\n",
    "    df[\"day\"] = df['request_datetime'].dt.day\n",
    "    df[\"week\"] = df['request_datetime'].dt.weekday\n",
    "    #df[\"month\"] = df['request_datetime'].dt.month\n",
    "\n",
    "    phone_no_m = df[[\"phone_no_m\"]].copy()\n",
    "    phone_no_m = phone_no_m.drop_duplicates(subset=['phone_no_m'], keep='last')\n",
    "    # 对话人数和对话次数\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"opposite_no_m\"].agg(sms_count=\"count\", sms_nunique=\"nunique\")\n",
    "    tmp[\"sms_rate\"] = tmp[\"sms_count\"]/tmp[\"sms_nunique\"]\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \"\"\"短信下行比例\n",
    "    \"\"\"\n",
    "    calltype2 = df[df[\"calltype_id\"]==2].copy()\n",
    "    calltype2 = calltype2.groupby(\"phone_no_m\")[\"calltype_id\"].agg(calltype_2=\"count\")\n",
    "    phone_no_m = phone_no_m.merge(calltype2, on=\"phone_no_m\", how=\"left\")\n",
    "    phone_no_m[\"calltype_rate\"] = phone_no_m[\"calltype_2\"] / phone_no_m[\"sms_count\"]\n",
    "    \"\"\"短信时间\n",
    "    \"\"\"\n",
    "    tmp = df.groupby(\"phone_no_m\")[\"hour\"].agg(hour_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               hour_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               hour_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby(\"phone_no_m\")[\"day\"].agg(day_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               day_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               day_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    tmp = df.groupby(\"phone_no_m\")[\"week\"].agg(hour_mode = lambda x:stats.mode(x)[0][0], \n",
    "                                               hour_mode_count = lambda x:stats.mode(x)[1][0], \n",
    "                                               hour_nunique=\"nunique\")\n",
    "    phone_no_m = phone_no_m.merge(tmp, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    return phone_no_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train_model():\n",
    "    print('Model')\n",
    "    test_app_feat=pd.read_csv('user_data/test_app_feat.csv')\n",
    "    test_voc_feat=pd.read_csv('user_data/test_voc_feat.csv')\n",
    "    test_sms_feat=pd.read_csv(\"user_data/test_sms_feat.csv\")\n",
    "    test_user    = pd.read_csv('user_data/test_user_feat.csv')\n",
    "    test_user = test_user.merge(test_app_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    test_user = test_user.merge(test_voc_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    test_user = test_user.merge(test_sms_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    \n",
    "    train_app_feat = pd.read_csv(\"user_data/train_app_feat.csv\")\n",
    "    train_voc_feat = pd.read_csv(\"user_data/train_voc_feat.csv\")\n",
    "    train_sms_feat = pd.read_csv(\"user_data/train_sms_feat.csv\")\n",
    "    train_user     = pd.read_csv('user_data/train_user_feat.csv')\n",
    "    \n",
    "    drop_r = [\"arpu_201908\",\"arpu_201909\",\"arpu_201910\",\"arpu_201911\",\"arpu_201912\",\"arpu_202001\",\"arpu_202002\"]\n",
    "    \n",
    "    # train_user['arpu_202004'] = train_user[drop_r].min(axis=1)\n",
    "    \n",
    "    train_user.drop(drop_r, axis=1,inplace=True)\n",
    "    train_user.rename(columns={\"arpu_202003\":\"arpu_202004\"},inplace=True)\n",
    "    \n",
    "    \n",
    "    train_user = train_user.merge(train_app_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    train_user = train_user.merge(train_voc_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    train_user = train_user.merge(train_sms_feat, on=\"phone_no_m\", how=\"left\")\n",
    "    sub = test_user[[\"phone_no_m\"]].copy()\n",
    "    \n",
    "    train_label = train_user[[\"label\"]].copy()\n",
    "    \n",
    "    test_user.drop([\"phone_no_m\"], axis=1,inplace=True)\n",
    "    train_user.drop([\"phone_no_m\", \"label\"], axis=1,inplace=True)\n",
    "    kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=1024)\n",
    "    test_p = []\n",
    "    threshold = 0\n",
    "    scores = 0\n",
    "    \n",
    "    \n",
    "#     train_user.fillna(0,inplace=True)\n",
    "#     test_user.fillna(0,inplace=True)\n",
    "\n",
    "    model = lgb.LGBMClassifier(n_estimators=1000)\n",
    "    # model = XGBClassifier()\n",
    "    # model = GradientBoostingClassifier(random_state=20200618,\n",
    "    #                                     learning_rate=0.1,\n",
    "    #                                     max_depth=7,\n",
    "    #                                     n_estimators=1000,\n",
    "                                       \n",
    "    #                                     )\n",
    "    for i, (train_index, vaild_index) in enumerate(kf.split(train_user, train_label[\"label\"])):\n",
    "        print('\\nFold_{} Training ================================\\n'.format(i+1))\n",
    "        train_x = train_user.iloc[train_index]\n",
    "        train_y = train_label.iloc[train_index]\n",
    "        valid_x = train_user.iloc[vaild_index]\n",
    "        valid_y = train_label.iloc[vaild_index]\n",
    "        \n",
    "        # LGB\n",
    "        model = model.fit(train_x,\n",
    "                          train_y, \n",
    "                          eval_names=['train', 'valid'],\n",
    "                          eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "                          early_stopping_rounds=10000,\n",
    "                          eval_metric='auc',\n",
    "                          categorical_feature=[],\n",
    "                          verbose=500\n",
    "                          )\n",
    "        \n",
    "        # XGB\n",
    "        # model = model.fit(train_x,train_y)\n",
    "        \n",
    "        \n",
    "        # GBDT\n",
    "        # model = model.fit(train_x, \n",
    "        #                   train_y,                          \n",
    "        #                   )\n",
    "        # model.score(valid_x, valid_y)\n",
    "        \n",
    "        vaild_preds = model.predict(valid_x, \n",
    "                                    num_iteration=model.best_iteration_\n",
    "                                  )\n",
    "        bp, bestf1= score_vail(vaild_preds, valid_y)\n",
    "        print(\"score: \", bestf1 , bp)\n",
    "        scores += bestf1\n",
    "        threshold += bp\n",
    "        test_pre = model.predict(test_user, \n",
    "                                num_iteration=model.best_iteration_\n",
    "                               )\n",
    "        test_pre = MinMaxScaler().fit_transform(test_pre.reshape(-1, 1))\n",
    "        test_pre = test_pre.reshape(-1, )\n",
    "        test_p.append(test_pre)\n",
    "    threshold = threshold/len(test_p)\n",
    "    print(threshold)\n",
    "    print(\"五折平均分数: \", scores/len(test_p))\n",
    "    sc = scores/len(test_p)\n",
    "    test_p = np.array(test_p)\n",
    "    test_p = test_p.mean(axis=0)\n",
    "    sub[\"prob\"] = test_p\n",
    "    sub[\"label\"] = sub[\"prob\"] > round(np.percentile(sub[\"prob\"], threshold), 4)\n",
    "    sub[[\"phone_no_m\", \"label\"]].to_csv('submissions/xgb_{}.csv'.format(sc),index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_vail(vaild_preds, real):\n",
    "    \"\"\"f1阈值搜索\n",
    "    \"\"\"\n",
    "#     import matplotlib.pylab as plt\n",
    "#     plt.figure(figsize=(16,5*10))\n",
    "    best = 0\n",
    "    bp = 0\n",
    "    score = []\n",
    "    for i in range(600):\n",
    "        p = 32+i*0.08\n",
    "        threshold_test = round(np.percentile(vaild_preds, p), 4)\n",
    "        pred_int = vaild_preds>threshold_test\n",
    "        ff = f1_score(pred_int,real)\n",
    "        score.append(ff)\n",
    "        \n",
    "        if ff>=best:\n",
    "            best = ff\n",
    "            bp = p\n",
    "#     plt.plot(range(len(score)), score)\n",
    "#     plt.show()\n",
    "    return bp, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats():\n",
    "    \n",
    "    test_voc=pd.read_csv('test/test_voc.csv',)\n",
    "    test_voc_feat = get_voc_feat(test_voc)\n",
    "    test_voc_feat.to_csv(\"user_data/test_voc_feat.csv\", index=False)\n",
    "\n",
    "    test_app=pd.read_csv('test/test_app.csv',)\n",
    "    test_app['month_id'] = '2020-03'\n",
    "    test_app_feat = get_app_feats(test_app)\n",
    "    test_app_feat.to_csv(\"user_data/test_app_feat.csv\", index=False)\n",
    "    \n",
    "    test_sms=pd.read_csv('test/test_sms.csv',)\n",
    "    test_sms_feat = get_sms_feats(test_sms)\n",
    "    test_sms_feat.to_csv(\"user_data/test_sms_feat.csv\", index=False)\n",
    "     \n",
    "    train_voc=pd.read_csv('train/train_voc.csv',)\n",
    "    train_voc_feat = get_voc_feat(train_voc)\n",
    "    train_voc_feat.to_csv(\"user_data/train_voc_feat.csv\", index=False)\n",
    "\n",
    "    train_app=pd.read_csv('train/train_app.csv',)\n",
    "    train_app_feat = get_app_feats(train_app)\n",
    "    train_app_feat.to_csv(\"user_data/train_app_feat.csv\", index=False)\n",
    "\n",
    "    train_sms=pd.read_csv('train/train_sms.csv',)\n",
    "    train_sms_feat = get_sms_feats(train_sms)\n",
    "    train_sms_feat.to_csv(\"user_data/train_sms_feat.csv\", index=False)\n",
    "    \n",
    "    train_user = pd.read_csv('train/train_user.csv')\n",
    "    test_user = pd.read_csv('test/test_user.csv')\n",
    "    train_user_feat, test_user_feat = get_user_feats(train_user, test_user)\n",
    "    train_user_feat.to_csv(\"user_data/train_user_feat.csv\", index=False)\n",
    "    test_user_feat.to_csv(\"user_data/test_user_feat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC\n",
      "APP\n",
      "SMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APP\n",
      "SMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model\n",
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 10000 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's auc: 1\ttrain's binary_logloss: 0.000298692\tvalid's auc: 0.970862\tvalid's binary_logloss: 0.348924\n",
      "[1000]\ttrain's auc: 1\ttrain's binary_logloss: 0.000285364\tvalid's auc: 0.972047\tvalid's binary_logloss: 0.407378\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[141]\ttrain's auc: 1\ttrain's binary_logloss: 0.0105144\tvalid's auc: 0.969475\tvalid's binary_logloss: 0.184055\n",
      "score:  0.9054054054054055 71.6\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 10000 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's auc: 1\ttrain's binary_logloss: 0.000292733\tvalid's auc: 0.955313\tvalid's binary_logloss: 0.485305\n",
      "[1000]\ttrain's auc: 1\ttrain's binary_logloss: 0.000285203\tvalid's auc: 0.955251\tvalid's binary_logloss: 0.562786\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[127]\ttrain's auc: 1\ttrain's binary_logloss: 0.0118361\tvalid's auc: 0.95538\tvalid's binary_logloss: 0.231246\n",
      "score:  0.8876712328767123 72.32\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 10000 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[500]\ttrain's auc: 1\ttrain's binary_logloss: 0.000295559\tvalid's auc: 0.95923\tvalid's binary_logloss: 0.459909\n",
      "[1000]\ttrain's auc: 1\ttrain's binary_logloss: 0.000285296\tvalid's auc: 0.959408\tvalid's binary_logloss: 0.53849\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[136]\ttrain's auc: 1\ttrain's binary_logloss: 0.0107576\tvalid's auc: 0.961947\tvalid's binary_logloss: 0.218963\n",
      "score:  0.890125173852573 73.2\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 10000 rounds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[500]\ttrain's auc: 1\ttrain's binary_logloss: 7.14129e-06\tvalid's auc: 0.956282\tvalid's binary_logloss: 0.495367\n",
      "[1000]\ttrain's auc: 1\ttrain's binary_logloss: 1.41684e-06\tvalid's auc: 0.956214\tvalid's binary_logloss: 0.567218\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[93]\ttrain's auc: 1\ttrain's binary_logloss: 0.0229136\tvalid's auc: 0.954943\tvalid's binary_logloss: 0.213066\n",
      "score:  0.8845618915159944 73.2\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "E:\\Python\\Anaconda\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10000 rounds\n",
      "[500]\ttrain's auc: 1\ttrain's binary_logloss: 9.95188e-06\tvalid's auc: 0.963356\tvalid's binary_logloss: 0.422107\n",
      "[1000]\ttrain's auc: 1\ttrain's binary_logloss: 1.52234e-06\tvalid's auc: 0.963885\tvalid's binary_logloss: 0.49094\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[108]\ttrain's auc: 1\ttrain's binary_logloss: 0.018931\tvalid's auc: 0.966648\tvalid's binary_logloss: 0.183391\n",
      "score:  0.9046979865771813 71.2\n",
      "72.304\n",
      "五折平均分数:  0.8944923380455734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
